{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, sqrt, sin, cos, log, inf\n",
    "import random\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_values(goal, starting_values):\n",
    "\n",
    "    x_goal, y_goal, z_goal = goal\n",
    "\n",
    "    t_1_int, d_2_int, d_3_int, t_4_int, t_5_int = starting_values\n",
    "    t_1, d_2, d_3, t_4, t_5 = starting_values\n",
    "\n",
    "    # Constant\n",
    "    t_6 = radians(40)\n",
    "    d_6 = .2\n",
    "\n",
    "    #Arbitrary\n",
    "    d_1 = 3\n",
    "    threshold = 0.001\n",
    "\n",
    "    x = (cos(t_1) * cos(t_4) * sin(t_5) * d_6) - (sin(t_1) * cos(t_5) * d_6) - (sin(t_1) * d_3)\n",
    "    y = (sin(t_1) * cos(t_4) * sin(t_5) * d_6) + (cos(t_1) * cos(t_5) * d_6) + (cos(t_1) * d_3)\n",
    "    z = (sin(t_4) * sin(t_5) * d_6) + d_1 + d_2\n",
    "\n",
    "    def dist(x1, x2, y1, y2, z1, z2):\n",
    "        return sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\n",
    "\n",
    "    change = 0\n",
    "    n = 1\n",
    "    while(n > 0):\n",
    "        \n",
    "        factor = dist(x, x_goal, y, y_goal, z, z_goal) * 2\n",
    "        t_1_diff = (random.random()-0.5) * factor\n",
    "        t_4_diff = (random.random()-0.5) * factor\n",
    "        t_5_diff = (random.random()-0.5) * factor\n",
    "        d_2_diff = (random.random()-0.5) * factor\n",
    "        d_3_diff = (random.random()-0.5) * factor\n",
    "\n",
    "        t_1_temp = t_1 - t_1_diff\n",
    "        t_4_temp = t_4 - t_4_diff\n",
    "        t_5_temp = t_5 - t_5_diff\n",
    "        d_2_temp = d_2 - d_2_diff\n",
    "        d_3_temp = d_3 - d_3_diff\n",
    "\n",
    "        change += abs(t_1_diff) + abs(t_4_diff) + abs(t_5_diff) + abs(d_2_diff) + abs(d_3_diff)\n",
    "\n",
    "        x_tmp = (cos(t_1_temp) * cos(t_4_temp) * sin(t_5_temp) * d_6) - (sin(t_1_temp) * cos(t_5_temp) * d_6) - (sin(t_1_temp) * d_3_temp)\n",
    "        y_tmp = (sin(t_1_temp) * cos(t_4_temp) * sin(t_5_temp) * d_6) + (cos(t_1_temp) * cos(t_5_temp) * d_6) + (cos(t_1_temp) * d_3_temp)\n",
    "        z_tmp = (sin(t_4_temp) * sin(t_5_temp) * d_6) + d_1 + d_2_temp\n",
    "\n",
    "        if abs(x_tmp - x_goal) < threshold and abs(y_tmp - y_goal) < threshold and abs(z_tmp - z_goal) < threshold:\n",
    "            # print(\"success\")\n",
    "            # print(f\"Final XYZ {x_tmp:.4}, {y_tmp:.4}, {z_tmp:.4},\")\n",
    "            # print(f\"Final thetas {t_1_temp=:.4} {t_4_temp=:.4} {t_5_temp=:.4} {d_2_temp=} {d_3_temp=}\")\n",
    "            # print(\"num iter {}\".format(n))\n",
    "            # print('Change: ', change)\n",
    "            return (x_tmp, y_tmp, z_tmp), (t_1_temp, d_2_temp, d_3_temp, t_4_temp, t_5_temp), change, n\n",
    "        \n",
    "        if dist(x_goal, x, y_goal, y, z_goal, z) < dist(x_goal, x_tmp, y_goal, y_tmp, z_goal, z_tmp):\n",
    "            pass\n",
    "        else:\n",
    "            t_1 = t_1_temp \n",
    "            t_4 = t_4_temp\n",
    "            t_5 = t_5_temp\n",
    "            d_2 = d_2_temp\n",
    "            d_3 = d_3_temp\n",
    "\n",
    "            x = x_tmp\n",
    "            y = y_tmp\n",
    "            z = z_tmp\n",
    "\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "Final XYZ: 1.2, 0.7999, 0.4994,\n",
      "Final values: t_1=-1.009 t_4=-4.502 t_5=2.016 d_2=-2.677064776479278 d_3=1.527912654593763\n",
      "Iterations: 98\n",
      "Change:  77.44061701195574\n"
     ]
    }
   ],
   "source": [
    "goal = (1.2, 0.8, 0.5)\n",
    "starting_values = (radians(-90), 0.5, 1, radians(-90), radians(90))\n",
    "position, values, change, iterations = compute_values(goal, starting_values)\n",
    "x, y, z = position\n",
    "t_1, d_2, d_3, t_4, t_5 = values\n",
    "\n",
    "print(\"success\")\n",
    "print(f\"Final XYZ: {x:.4}, {y:.4}, {z:.4},\")\n",
    "print(f\"Final values: {t_1=:.4} {t_4=:.4} {t_5=:.4} {d_2=} {d_3=}\")\n",
    "print(\"Iterations: {}\".format(iterations))\n",
    "print('Change: ', change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we came up with after trying for too long to solve it using a neural network RL approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "Final XYZ: 1.2, 0.8, 0.5004,\n",
      "Final values: t_1 = -0.896 t_4 = -0.674 t_5 = 0.9246 d_2 = -2.4 d_3 = 1.316\n",
      "Iterations: 63\n",
      "Change:  8.111665934888014\n"
     ]
    }
   ],
   "source": [
    "min_change = inf\n",
    "saved = None\n",
    "\n",
    "goal = (1.2, 0.8, 0.5)\n",
    "starting_values = (radians(0), 0.2, .3, radians(-90), radians(90))\n",
    "\n",
    "for iteration in range(1, 100000):\n",
    "    position, values, change, iterations = compute_values(goal, starting_values)\n",
    "    if change < min_change:\n",
    "        min_change = change\n",
    "        saved = (position, values, change, iterations)\n",
    "\n",
    "\n",
    "position, values, change, iteration = saved\n",
    "x, y, z = position\n",
    "t_1, d_2, d_3, t_4, t_5 = values\n",
    "\n",
    "print(\"success\")\n",
    "print(f\"Final XYZ: {x:.4}, {y:.4}, {z:.4},\")\n",
    "print(f\"Final values: {t_1 = :.4} {t_4 = :.4} {t_5 = :.4} {d_2 = :.4} {d_3 = :.4}\")\n",
    "print(\"Iterations: {}\".format(iterations))\n",
    "print('Change: ', change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to train a neural network to do this but it had some serious issues. I think we approached the problem wrong. Below is our initial attempt at 3b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, max_size=100):\n",
    "        self.memory = deque(maxlen=max_size)\n",
    "\n",
    "    def push(self, element):\n",
    "        self.memory.append(element)\n",
    "\n",
    "    def get_batch(self, batch_size=4):\n",
    "        if batch_size > len(self.memory):\n",
    "            batch_size = len(self.memory)\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Current elements in memory: {len(self.memory)}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_goal = 1.2\n",
    "        self.y_goal = 0.8\n",
    "        self.z_goal = 0.5\n",
    "        # Constant\n",
    "        self.d_6 = .2\n",
    "        self.d_1 = 3\n",
    "        self.threshold = 0.01\n",
    "\n",
    "        self.positions = (radians(0),\n",
    "                    0.2,\n",
    "                    .3,\n",
    "                    radians(-90),\n",
    "                    radians(90))\n",
    "        self.x = inf\n",
    "        self.y = inf\n",
    "        self.z = inf\n",
    "\n",
    "    def calculatePosition(self, positional_info):\n",
    "        t_1, d_2, d_3, t_4, t_5 = positional_info\n",
    "        \n",
    "        \n",
    "        x = (cos(t_1) * cos(t_4) * sin(t_5) * self.d_6) - (sin(t_1) * cos(t_5) * self.d_6) - (sin(t_1) * d_3)\n",
    "        y = (sin(t_1) * cos(t_4) * sin(t_5) * self.d_6) + (cos(t_1) * cos(t_5) * self.d_6) + (cos(t_1) * d_3)\n",
    "        z = (sin(t_4) * sin(t_5) * self.d_6) + self.d_1 + d_2\n",
    "\n",
    "        return (x, y, z)\n",
    "\n",
    "    def step(self, positional_info, steps):\n",
    "        # self.positions = positional_info\n",
    "        x_new, y_new, z_new = self.calculatePosition(positional_info)\n",
    "        x_current, y_current, z_current = self.calculatePosition(self.positions)\n",
    "\n",
    "        new_distance_to_end = self.dist(x_new, self.x_goal, y_new, self.y_goal, z_new, self.z_goal)\n",
    "        current_distance_to_end = self.dist(x_current, self.x_goal, y_current, self.y_goal, z_current, self.z_goal)\n",
    "\n",
    "        if new_distance_to_end < self.threshold:\n",
    "            self.positions = positional_info\n",
    "            done = True\n",
    "            reward = 500 - steps if steps < 500 else 0\n",
    "        elif new_distance_to_end < current_distance_to_end:\n",
    "            self.positions = positional_info\n",
    "            done = False\n",
    "            reward = 0\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 0\n",
    "            \n",
    "        return (done, np.array(self.positions), reward)\n",
    "\n",
    "    def dist(self, x1, x2, y1, y2, z1, z2):\n",
    "        return sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = (radians(0),\n",
    "                    0.2,\n",
    "                    .3,\n",
    "                    radians(-90),\n",
    "                    radians(90))\n",
    "\n",
    "        return (False, np.array(self.positions), 0)\n",
    "\n",
    "    def getPositions(self):\n",
    "        return self.positions\n",
    "\n",
    "    def getDistanceFromGoal(self):\n",
    "        x, y, z = self.calculatePosition(self.positions)\n",
    "        return self.dist(x, self.x_goal, y, self.y_goal, z, self.z_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(5, 5)\n",
    "        self.layer2 = torch.nn.Linear(5, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        layer1_output = self.layer1(input)\n",
    "\n",
    "        layer1_relu = torch.nn.functional.relu(layer1_output)\n",
    "\n",
    "        layer2_output = self.layer2(layer1_relu)\n",
    "\n",
    "        return layer2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = NN().to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=3e-3)\n",
    "        self.loss_function = torch.nn.MSELoss()\n",
    "        \n",
    "        self.decay = 0.995\n",
    "        self.randomness = 1.0\n",
    "        self.min_randomness = 0.01\n",
    "        self.move_step_size = 1.0\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).to(self.device)\n",
    "        probs = self.model(state.float()).cpu()\n",
    "        \n",
    "        if np.random.rand() <= self.randomness:\n",
    "            action = np.random.randint(low=0, high=probs.size(dim=0))\n",
    "        else:\n",
    "            action = torch.argmax(probs).item()\n",
    "\n",
    "        if action > 4:\n",
    "            state[action - 5] -= self.move_step_size\n",
    "        else:\n",
    "            state[action] += self.move_step_size\n",
    "\n",
    "        return action, state.numpy()\n",
    "\n",
    "    def update(self, memories):\n",
    "        states, next_states, actions, rewards = self.unpack_batch(memories)\n",
    "\n",
    "        old_targets = self.old_targets(states, actions)\n",
    "        new_targets = self.new_targets(states, next_states, rewards, actions)\n",
    "\n",
    "        loss = torch.nn.functional.smooth_l1_loss(old_targets, new_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def unpack_batch(self, batch):\n",
    "        states, next_states, actions, rewards = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states).float().to(self.device)\n",
    "        next_states = torch.tensor(next_states).float().to(self.device)\n",
    "\n",
    "        actions = torch.tensor(actions).unsqueeze(1).long().to(self.device)\n",
    "        rewards = torch.tensor(rewards).unsqueeze(1).float().to(self.device)\n",
    "\n",
    "        return states, next_states, actions, rewards\n",
    "\n",
    "    def old_targets(self, states, actions):\n",
    "        return self.model(states).gather(1, actions)\n",
    "\n",
    "    def new_targets(self, states, next_states, rewards, actions):\n",
    "        return rewards + torch.amax(self.model(next_states), dim=1, keepdim=True)\n",
    "\n",
    "    def update_randomness(self):\n",
    "        self.randomness *= self.decay\n",
    "        self.randomness = max(self.randomness, self.min_randomness)\n",
    "    \n",
    "    def update_move_step(self, distance):\n",
    "        self.move_step_size = distance / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_21692\\2498297430.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  states = torch.tensor(states).float().to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration: 50\n",
      "  Average Final Distance: 0.0076\n",
      "  Average Steps: 99.1200\n",
      "  Average Loss: 3.8565\n",
      "  Agent Randomness: 0.778\n",
      "\n",
      "  Iteration: 100\n",
      "  Average Final Distance: 0.0077\n",
      "  Average Steps: 124.3800\n",
      "  Average Loss: 4.1295\n",
      "  Agent Randomness: 0.606\n",
      "\n",
      "  Iteration: 150\n",
      "  Average Final Distance: 0.0075\n",
      "  Average Steps: 159.5600\n",
      "  Average Loss: 3.1726\n",
      "  Agent Randomness: 0.471\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m     33\u001b[0m     memory_batch \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39mget_batch(batch_size\u001b[39m=\u001b[39m\u001b[39m400\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     loss \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mupdate(memory_batch)\n\u001b[0;32m     35\u001b[0m losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     36\u001b[0m agent\u001b[39m.\u001b[39mupdate_randomness()\n",
      "Cell \u001b[1;32mIn [21], line 39\u001b[0m, in \u001b[0;36mAgent.update\u001b[1;34m(self, memories)\u001b[0m\n\u001b[0;32m     36\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msmooth_l1_loss(old_targets, new_targets)\n\u001b[0;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 39\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Jonathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Jonathan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent() \n",
    "env = Environment()\n",
    "memory = Memory(max_size=30000)\n",
    "\n",
    "max_iteration = 1000\n",
    "logging_iteration = 50\n",
    "learning = []\n",
    "losses = []\n",
    "distances = []\n",
    "\n",
    "for iteration in range(1, max_iteration - 1):\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    done, state, reward = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action, position = agent.act(state)\n",
    "        done, next_state, reward = env.step(position, steps)\n",
    "        memory.push(element=(state, next_state, action, reward))\n",
    "\n",
    "        agent.update_move_step(env.getDistanceFromGoal())\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            distances.append(env.getDistanceFromGoal())\n",
    "\n",
    "\n",
    "    for _ in range(100):\n",
    "        memory_batch = memory.get_batch(batch_size=400)\n",
    "        loss = agent.update(memory_batch)\n",
    "    losses.append(loss)\n",
    "    agent.update_randomness()\n",
    "    learning.append(steps)\n",
    "\n",
    "    \n",
    "    if iteration % logging_iteration == 0:\n",
    "        print(f\"  Iteration: {iteration}\")\n",
    "        print(f\"  Average Final Distance: {np.mean(distances[-logging_iteration:]):.4f}\")\n",
    "        print(f\"  Average Steps: {np.mean(learning[-logging_iteration:]):.4f}\")\n",
    "        print(f\"  Average Loss: {np.mean(losses[-logging_iteration:]):.4f}\")\n",
    "        print(f\"  Agent Randomness: {agent.randomness:.3f}\")\n",
    "        print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5252f41cda903ea636bae5f212409c6b52ab532911ebf5c960538b6c783ba1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
